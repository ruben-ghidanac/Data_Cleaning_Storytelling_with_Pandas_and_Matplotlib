{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGfHi2emuFvJ"
      },
      "source": [
        "# Grouping and aggregating data\n",
        "\n",
        "Most of the time, in pandas, we use `.groupby()` to group and aggregate data. This is a function that splits your data into certain categories, and then performs an aggregation, transformation, or filtration to those categories. Today we will only be looking at `.groupby()` and aggregations, and how they can help us to analyse our data. We will also learn about `resample()` as an alternative, more concise way to group dates and times.\n",
        "\n",
        "As always, let's start by reading the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lq7zsZka4q2m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "30IJsE7t49Eu"
      },
      "outputs": [],
      "source": [
        "# orders_cl.csv\n",
        "path = \"C:/Users/ruben/WBS/Data_Science/Section_3/Data_Cleaning_Storytelling_with_Pandas_and_Matplotlib/data/clean_data/orders_cl.csv\"\n",
        "orders_cl = pd.read_csv(path)\n",
        "\n",
        "# orderlines_cl.csv\n",
        "path = \"C:/Users/ruben/WBS/Data_Science/Section_3/Data_Cleaning_Storytelling_with_Pandas_and_Matplotlib/data/clean_data/orderlines_cl.csv\"\n",
        "orderlines_cl = pd.read_csv(path)\n",
        "\n",
        "# products_cl.csv\n",
        "path = \"C:/Users/ruben/WBS/Data_Science/Section_3/Data_Cleaning_Storytelling_with_Pandas_and_Matplotlib/data/clean_data/products_cl.csv\"\n",
        "products_cl = pd.read_csv(path)\n",
        "\n",
        "# brands_cl.csv\n",
        "path = \"C:/Users/ruben/WBS/Data_Science/Section_3/Data_Cleaning_Storytelling_with_Pandas_and_Matplotlib/data/clean_data/brands_cl.csv\"\n",
        "brands_cl = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "97-FGws5RzTk"
      },
      "outputs": [],
      "source": [
        "ol_df = orderlines_cl.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sD3zRjTyk-9h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 216250 entries, 0 to 216249\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count   Dtype  \n",
            "---  ------            --------------   -----  \n",
            " 0   id                216250 non-null  int64  \n",
            " 1   id_order          216250 non-null  int64  \n",
            " 2   product_id        216250 non-null  int64  \n",
            " 3   product_quantity  216250 non-null  int64  \n",
            " 4   sku               216250 non-null  object \n",
            " 5   unit_price        216250 non-null  float64\n",
            " 6   date              216250 non-null  object \n",
            "dtypes: float64(1), int64(4), object(2)\n",
            "memory usage: 11.5+ MB\n"
          ]
        }
      ],
      "source": [
        "ol_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5JVxtyQnzLV"
      },
      "source": [
        "Change `date` to datetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxwG7CJ9lIQB"
      },
      "outputs": [],
      "source": [
        "ol_df[\"date\"] = pd.to_datetime(ol_df[\"date\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXWNhA9C2KsO"
      },
      "source": [
        "## 1.&nbsp; `.groupby` a single feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuxv5HPeUcJp"
      },
      "source": [
        "When we use the `.groupby()` method, we always state in the parentheses the name of the column that contains the categories that should be grouped. For example, we are grouping here by the product number (SKU), which will give us one line per sku. `.count()` will tell us how many rows have been condensed into each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WAJ97U7Ryp8"
      },
      "outputs": [],
      "source": [
        "ol_df.groupby(\"sku\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpu853EhFWD2"
      },
      "source": [
        "Notice how in this \"grouped\" DataFrame we have 6798 rows: one row for each unique `sku`. In the original DataFrame we had many more rows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRrHMdH1Fgpf"
      },
      "outputs": [],
      "source": [
        "ol_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXx21qYyVM76"
      },
      "source": [
        "As you can see above, the same number of rows were condensed for each column, this is to be expected when we use `count()` as the aggregation function. However, if we use an aggregate such as `.sum()`, then we get the total for the condensed values of each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJhFi7a3Vr7c"
      },
      "outputs": [],
      "source": [
        "ol_df.groupby(\"sku\").sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxpruL7bV26a"
      },
      "source": [
        "Now, obviously, not all of the information above matters: it's not relevant what the sum of the `id`'s is. So, let's select the `product_quantity` and see how many units of each product were sold:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "156UzFjzSpuS"
      },
      "outputs": [],
      "source": [
        "ol_df.groupby(\"sku\")[\"product_quantity\"].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGyvEWtKWiBn"
      },
      "source": [
        "It can be useful sometimes to sort the values from highest to lowest. With the `.sum()` of `product_quantity` this will help us to see how many of each item we sold in order of quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A99m-tRCT9RK"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df\n",
        "    .groupby(\"sku\")[\"product_quantity\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D58-EP0Snmi-"
      },
      "source": [
        "Or if you wish for only a certain number of the largest of smallest numbers, remember you can use `.nlargest()` or `.nsmallest()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPKxQPDen0tO"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df\n",
        "    .groupby(\"sku\")[\"product_quantity\"]\n",
        "    .sum()\n",
        "    .nlargest(5)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8fNKlbMkC9c"
      },
      "source": [
        "### 1.1.&nbsp; List of aggregates\n",
        "\n",
        "It is possible to use many different aggregates besides just `.sum()` and `.count()`. A full list can be found [here](https://pandas.pydata.org/docs/reference/groupby.html), and below are a few of the most useful.\n",
        "\n",
        "* `.count()` – Number of non-null observations\n",
        "* `.sum()` – Sum of values\n",
        "* `.mean()` – Mean of values\n",
        "* `.median()` – Median of values\n",
        "* `.min()` – Minimum\n",
        "* `.max()` – Maximum\n",
        "* `.std()` – Standard deviation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hCaahyEki_6"
      },
      "source": [
        "### 1.2.&nbsp; `.agg()`\n",
        "We are not restricted to only using one aggregate at a time. We can use the `.agg()` method to see multiple aggregates at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4uueTuf_Ypk"
      },
      "source": [
        "#### 1.2.1.&nbsp; multiple aggregates for one column\n",
        "Here we first `.groupby()` the `sku`'s, then take the column `product quantity`, and look at both the `.sum()` and `.count()` of `product_quantity`. We also here reduced our DataFrame to the 10 largest `sku`\"s according to the `.sum()` of `product_quantity`.\n",
        "\n",
        "> **Tip:** when working with many Pandas methods chained one after the other, wrap all the statement in parenthesis and add a line break before any `.method()`. It will improve the readability of the code*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMktUr14kGv0"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df\n",
        "    .groupby(\"sku\")[\"product_quantity\"]\n",
        "    .agg([\"sum\", \"count\"])\n",
        "    .nlargest(10, \"sum\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TqpcLupNU7n"
      },
      "source": [
        "#### 1.2.2.&nbsp; mutliple aggregates for more than one column\n",
        "Not only can we look at multiple aggregates for one column. We can also look at multiple aggregates over multiple columns. Here we again `.groupby()` `sku` and take the `.sum()` and `.count()` of `product_quantity`, **and also** look at the `.mean()` of the `unit_price`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGPFDBBl35VV"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df\n",
        "    .groupby(\"sku\")\n",
        "    .agg({\"product_quantity\": [\"sum\", \"count\"], \"unit_price\": \"mean\"})\n",
        "    .nlargest(10, (\"product_quantity\", \"sum\"))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtQqcHUj2bxn"
      },
      "source": [
        "## 2.&nbsp; `.groupby()` multiple features\n",
        "It's also possible to `.groupby()` multiple categories. Let's see how many products we sold each month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx3zB61ZX_NM"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df\n",
        "    .groupby([ol_df[\"date\"].dt.year, \n",
        "              ol_df[\"date\"].dt.month])[\"product_quantity\"]\n",
        "    .sum()\n",
        " )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eCJ-0YCqdMH"
      },
      "source": [
        "Looks like the end of the year and the begining of the year are the busiest times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhSTALCdHNs4"
      },
      "source": [
        "### 1.4.&nbsp; `.groupby()` aggregate plotting\n",
        "It's also possible to quickly plot aggregates with `.groupby()`, which can help us better understand the data. Let's plot the above values showing how many products we sold each month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYcSi8rbvqkc"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df.groupby([ol_df[\"date\"].dt.year, \n",
        "                   ol_df[\"date\"].dt.month])[\"product_quantity\"]\n",
        "    .sum()\n",
        "    .plot(figsize=(16, 12))\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LDe4vd8wOJS"
      },
      "source": [
        "## 3.&nbsp; `.resample()`\n",
        "\n",
        "Resampling allows us to easily aggregate time series data. When using `.resample()`, we have to define:\n",
        "- The frequency of time for which to group the data (e.g. 5 minutely, hourly, daily...) expressed as a \"frequency string\" (`\"W\"` for weekly, `\"M\"` for monthly...). Find all available frequency strings [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects).\n",
        "- The column with the dates group by needs to be passed to the `on` parameter (unless you want group by the index).\n",
        "- The aggregate function (`sum()`, `count()`, `mean()`...)\n",
        "- The numerical column/s to aggregate.\n",
        "\n",
        "With this we can easily recreate our groupby above, where we looked at the amount of products sold each month. The date displayed is the last day of the given month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8hb5n2zwfct"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df\n",
        "    .resample(\"M\", on=\"date\")[\"product_quantity\"]\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL0K2INaC5TC"
      },
      "source": [
        "We can also quickly plot a daily graph for the amount of products sold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJylW2glwdge"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df\n",
        "    .resample(\"D\", on=\"date\")\n",
        "    .sum()[\"product_quantity\"]\n",
        "    .plot(figsize=(12, 8))\n",
        " );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFFTrGagraNu"
      },
      "source": [
        "We can create the same graph as above using `.groupby()`, it just takes a little more code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7LQ7w6Qrlnb"
      },
      "outputs": [],
      "source": [
        "(\n",
        "    ol_df\n",
        "    .groupby([ol_df.date.dt.year, \n",
        "              ol_df.date.dt.month,\n",
        "              ol_df.date.dt.day])[\"product_quantity\"]\n",
        "    .sum()\n",
        "    .plot(figsize=(12, 8))\n",
        " );"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMBquCgarx0E"
      },
      "source": [
        "As you can see from the x-axis of the two charts above, `.resample()` works a little more inuitvely when you want to group by datetimes. This isn't to say that you cannot use `.groupby()` with datetimes, of course you can. `.resample()` just provides us with a little extra flexibility to group datetime by specific groups. For example, you can create `.resample()` input strings such as `2H20min` for groups of 2 hours 20minutes, `3min` for groups of 3 minutes, `1D3H` for 1 day 3hours etc... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m050CtFNk0aG"
      },
      "source": [
        "# Challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzK9J3B2mWyT"
      },
      "source": [
        "## Challenge 1. What are the top 8 most sold products?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykMv1QQN5Ujx"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXYMOnwWyfkf"
      },
      "source": [
        "## Challenge 2. How many products did each of the 5 largest orders contain?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EStMw8mQ5hkz"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LhlPkykEOZT"
      },
      "source": [
        "## Challenge 3. We want to see how busy the days of the week are to know when we need more staff. Rank the days of the week from busiest to least busiest. \n",
        "Bonus points for using your pandas plotting skills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJFw9Tmrvswv"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO2HC3vYzBB3"
      },
      "source": [
        "## Challenge 4. Which brands sell the most products?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lPd1Y_ZIhkD"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwvy0aVmQl_v"
      },
      "source": [
        "## Challenge 5. Plot a line chart for the amount of orders each hour of November 2017"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTYN3cYAIjSr"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "704aea7029d59a45433ae3b663f7cccf597e59967cc6a516676256c90bebdeef"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
